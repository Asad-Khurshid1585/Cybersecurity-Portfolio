"""
Generate embeddings for vulnerability descriptions using transformer models.
"""

import logging
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel
import torch
from pathlib import Path
import os
import pickle

logger = logging.getLogger(__name__)

class EmbeddingGenerator:
    """Generate embeddings for text using transformer models."""
    
    def __init__(self, model_name='all-MiniLM-L6-v2', cache_dir='models/embeddings',
                 device=None):
        """
        Initialize the embedding generator.
        
        Args:
            model_name: Name of the model to use
            cache_dir: Directory to cache embeddings
            device: Device to use for computations ('cpu', 'cuda', etc.)
        """
        self.model_name = model_name
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Determine device
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        logger.info(f"Loading embedding model {model_name} on {self.device}")
        try:
            self.model = SentenceTransformer(model_name, device=self.device)
            logger.info(f"Loaded embedding model: {model_name}")
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise
    
    def generate_embedding(self, text):
        """
        Generate embedding for a single text.
        
        Args:
            text: Input text
            
        Returns:
            Numpy array with embedding
        """
        if not text or not isinstance(text, str):
            logger.warning(f"Invalid input text: {text}")
            # Return zero vector of appropriate size
            return np.zeros(self.model.get_sentence_embedding_dimension())
        
        try:
            embedding = self.model.encode(text)
            return embedding
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            return np.zeros(self.model.get_sentence_embedding_dimension())
    
    def generate_embeddings(self, texts, batch_size=32):
        """
        Generate embeddings for multiple texts.
        
        Args:
            texts: List of input texts
            batch_size: Batch size for processing
            
        Returns:
            Numpy array with embeddings
        """
        if not texts:
            return np.array([])
        
        try:
            embeddings = self.model.encode(texts, batch_size=batch_size)
            return embeddings
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            # Return zero vectors of appropriate size
            return np.zeros((len(texts), self.model.get_sentence_embedding_dimension()))
    
    def cache_embeddings(self, texts, ids, cache_file=None):
        """
        Generate and cache embeddings for texts.
        
        Args:
            texts: List of texts
            ids: List of corresponding IDs
            cache_file: Filename for cache
            
        Returns:
            Dictionary mapping IDs to embeddings
        """
        if not cache_file:
            cache_file = f"{self.model_name.replace('/', '_')}_embeddings.pkl"
        
        cache_path = self.cache_dir / cache_file
        
        # Check if cache exists
        embedding_dict = {}
        if cache_path.exists():
            try:
                with open(cache_path, 'rb') as f:
                    embedding_dict = pickle.load(f)
                logger.info(f"Loaded {len(embedding_dict)} embeddings from cache")
            except Exception as e:
                logger.error(f"Error loading cache: {e}")
        
        # Determine which texts need embedding
        new_texts = []
        new_ids = []
        for text_id, text in zip(ids, texts):
            if text_id not in embedding_dict:
                new_texts.append(text)
                new_ids.append(text_id)
        
        if new_texts:
            logger.info(f"Generating {len(new_texts)} new embeddings")
            new_embeddings = self.generate_embeddings(new_texts)
            
            # Add new embeddings to dictionary
            for new_id, embedding in zip(new_ids, new_embeddings):
                embedding_dict[new_id] = embedding
            
            # Save updated cache
            with open(cache_path, 'wb') as f:
                pickle.dump(embedding_dict, f)
            logger.info(f"Saved {len(embedding_dict)} embeddings to cache")
        
        return embedding_dict

if __name__ == "__main__":
    # Example usage
    generator = EmbeddingGenerator()
    sample_texts = [
        "A buffer overflow vulnerability in the HTTP/2 implementation",
        "Cross-site scripting (XSS) vulnerability in login page"
    ]
    embeddings = generator.generate_embeddings(sample_texts)
    print(f"Generated embeddings shape: {embeddings.shape}")
