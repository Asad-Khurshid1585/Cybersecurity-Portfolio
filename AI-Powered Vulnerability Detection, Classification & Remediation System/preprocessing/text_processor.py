"""
Text processing module for vulnerability descriptions.
"""

import re
import string
import logging
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk

logger = logging.getLogger(__name__)

class TextProcessor:
    """Process vulnerability text descriptions."""
    
    def __init__(self):
        """Initialize the text processor."""
        # Download required NLTK resources if not already downloaded
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
        
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('stopwords')
        
        try:
            nltk.data.find('corpora/wordnet')
        except LookupError:
            nltk.download('wordnet')
        
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()
    
    def clean_text(self, text):
        """
        Clean the text by removing special characters, digits, etc.
        
        Args:
            text: Input text
            
        Returns:
            Cleaned text
        """
        if not text:
            return ""
        
        # Convert to lowercase
        text = text.lower()
        
        # Remove URLs
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        
        # Remove HTML tags
        text = re.sub(r'<.*?>', '', text)
        
        # Remove punctuation
        text = text.translate(str.maketrans('', '', string.punctuation))
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def tokenize(self, text):
        """
        Tokenize text into words.
        
        Args:
            text: Input text
            
        Returns:
            List of tokens
        """
        if not text:
            return []
        
        return word_tokenize(text)
    
    def remove_stopwords(self, tokens):
        """
        Remove stopwords from tokens.
        
        Args:
            tokens: List of tokens
            
        Returns:
            Filtered list of tokens
        """
        return [token for token in tokens if token not in self.stop_words]
    
    def lemmatize(self, tokens):
        """
        Lemmatize tokens.
        
        Args:
            tokens: List of tokens
            
        Returns:
            Lemmatized list of tokens
        """
        return [self.lemmatizer.lemmatize(token) for token in tokens]
    
    def process_text(self, text, remove_stopwords=True, lemmatize=True):
        """
        Process text by cleaning, tokenizing, and optionally removing stopwords and lemmatizing.
        
        Args:
            text: Input text
            remove_stopwords: Whether to remove stopwords
            lemmatize: Whether to lemmatize tokens
            
        Returns:
            Processed tokens
        """
        if not text:
            return []
        
        # Clean text
        cleaned_text = self.clean_text(text)
        
        # Tokenize
        tokens = self.tokenize(cleaned_text)
        
        # Remove stopwords
        if remove_stopwords:
            tokens = self.remove_stopwords(tokens)
        
        # Lemmatize
        if lemmatize:
            tokens = self.lemmatize(tokens)
        
        return tokens
    
    def prepare_for_embedding(self, text):
        """
        Prepare text for embedding models by minimal preprocessing.
        
        Args:
            text: Input text
            
        Returns:
            Processed text ready for embedding
        """
        if not text:
            return ""
        
        # Just clean up whitespace and HTML
        text = re.sub(r'<.*?>', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def extract_keywords(self, text, n=10):
        """
        Extract keywords from text based on TF-IDF-like approach.
        
        Args:
            text: Input text
            n: Number of keywords to extract
            
        Returns:
            List of keywords
        """
        # First process the text
        tokens = self.process_text(text)
        
        if not tokens:
            return []
        
        # Count token frequencies
        freq = {}
        for token in tokens:
            freq[token] = freq.get(token, 0) + 1
        
        # Sort by frequency
        sorted_tokens = sorted(freq.items(), key=lambda x: x[1], reverse=True)
        
        # Return top n keywords
        return [token for token, count in sorted_tokens[:n]]

if __name__ == "__main__":
    # Example usage
    processor = TextProcessor()
    sample_text = """
    A buffer overflow vulnerability in the HTTP/2 implementation of NGINX allows attackers 
    to execute arbitrary code by sending crafted HTTP/2 requests.
    """
    processed = processor.process_text(sample_text)
    print("Processed tokens:", processed)
    print("Keywords:", processor.extract_keywords(sample_text))
