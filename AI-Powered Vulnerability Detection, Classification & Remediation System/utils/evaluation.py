"""
Evaluation utilities for model performance assessment.
"""

import logging
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support,
    confusion_matrix, classification_report,
    mean_squared_error, r2_score
)
import seaborn as sns
from pathlib import Path

logger = logging.getLogger(__name__)

class ModelEvaluator:
    """Evaluate model performance with various metrics."""
    
    def __init__(self, output_dir='results'):
        """
        Initialize the model evaluator.
        
        Args:
            output_dir: Directory to save evaluation results
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def evaluate_classifier(self, y_true, y_pred, class_names=None, model_name='classifier'):
        """
        Evaluate a classification model.
        
        Args:
            y_true: True labels
            y_pred: Predicted labels
            class_names: List of class names
            model_name: Name of the model for reports
            
        Returns:
            Dictionary with evaluation metrics
        """
        if not class_names:
            class_names = sorted(list(set(y_true) | set(y_pred)))
        
        # Calculate metrics
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average='weighted'
        )
        
        # Generate classification report
        report = classification_report(y_true, y_pred, target_names=class_names)
        
        # Generate confusion matrix
        cm = confusion_matrix(y_true, y_pred, labels=class_names)
        
        # Plot confusion matrix
        self._plot_confusion_matrix(cm, class_names, model_name)
        
        # Save report to file
        report_path = self.output_dir / f"{model_name}_report.txt"
        with open(report_path, 'w') as f:
            f.write(f"Model: {model_name}\n")
            f.write(f"Accuracy: {accuracy:.4f}\n")
            f.write(f"Precision: {precision:.4f}\n")
            f.write(f"Recall: {recall:.4f}\n")
            f.write(f"F1 Score: {f1:.4f}\n\n")
            f.write("Classification Report:\n")
            f.write(report)
        
        logger.info(f"Saved classification report to {report_path}")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'confusion_matrix': cm,
            'report': report
        }
    
    def evaluate_regressor(self, y_true, y_pred, model_name='regressor'):
        """
        Evaluate a regression model.
        
        Args:
            y_true: True values
            y_pred: Predicted values
            model_name: Name of the model for reports
            
        Returns:
            Dictionary with evaluation metrics
        """
        # Calculate metrics
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)
        
        # Plot actual vs predicted
        self._plot_regression_results(y_true, y_pred, model_name)
        
        # Save report to file
        report_path = self.output_dir / f"{model_name}_report.txt"
        with open(report_path, 'w') as f:
            f.write(f"Model: {model_name}\n")
            f.write(f"Mean Squared Error: {mse:.4f}\n")
            f.write(f"Root Mean Squared Error: {rmse:.4f}\n")
            f.write(f"RÂ² Score: {r2:.4f}\n")
        
        logger.info(f"Saved regression report to {report_path}")
        
        return {
            'mse': mse,
            'rmse': rmse,
            'r2': r2
        }
    
    def evaluate_text_generation(self, reference_texts, generated_texts, model_name='text_generator'):
        """
        Evaluate text generation quality.
        
        Args:
            reference_texts: List of reference texts
            generated_texts: List of generated texts
            model_name: Name of the model for reports
            
        Returns:
            Dictionary with evaluation metrics
        """
        try:
            from rouge import Rouge
            from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
            import nltk
            
            # Download NLTK data if not already downloaded
            try:
                nltk.data.find('tokenizers/punkt')
            except LookupError:
                nltk.download('punkt')
            
            # Initialize metrics
            rouge = Rouge()
            smoother = SmoothingFunction().method1
            
            # Calculate ROUGE scores
            rouge_scores = rouge.get_scores(generated_texts, reference_texts, avg=True)
            
            # Calculate BLEU scores
            bleu_scores = []
            for ref, gen in zip(reference_texts, generated_texts):
                reference_tokens = [nltk.word_tokenize(ref)]
                generated_tokens = nltk.word_tokenize(gen)
                bleu = sentence_bleu(reference_tokens, generated_tokens, smoothing_function=smoother)
                bleu_scores.append(bleu)
            
            avg_bleu = np.mean(bleu_scores)
            
            # Save report to file
            report_path = self.output_dir / f"{model_name}_report.txt"
            with open(report_path, 'w') as f:
                f.write(f"Model: {model_name}\n")
                f.write(f"Average BLEU Score: {avg_bleu:.4f}\n\n")
                f.write("ROUGE Scores:\n")
                for metric, scores in rouge_scores.items():
                    f.write(f"  {metric}:\n")
                    for score_name, score_value in scores.items():
                        f.write(f"    {score_name}: {score_value:.4f}\n")
            
            logger.info(f"Saved text generation report to {report_path}")
            
            return {
                'bleu': avg_bleu,
                'rouge': rouge_scores
            }
            
        except ImportError:
            logger.warning("Rouge or NLTK packages not installed. Cannot evaluate text generation.")
            return {}
    
    def _plot_confusion_matrix(self, cm, class_names, model_name):
        """
        Plot and save confusion matrix.
        
        Args:
            cm: Confusion matrix
            class_names: List of class names
            model_name: Name of the model for the filename
        """
        plt.figure(figsize=(10, 8))
        sns.heatmap(
            cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names
        )
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.title(f'Confusion Matrix - {model_name}')
        
        plot_path = self.output_dir / f"{model_name}_confusion_matrix.png"
        plt.savefig(plot_path)
        plt.close()
        
        logger.info(f"Saved confusion matrix to {plot_path}")
    
    def _plot_regression_results(self, y_true, y_pred, model_name):
        """
        Plot and save regression results.
        
        Args:
            y_true: True values
            y_pred: Predicted values
            model_name: Name of the model for the filename
        """
        plt.figure(figsize=(10, 8))
        plt.scatter(y_true, y_pred, alpha=0.5)
        
        # Plot perfect prediction line
        min_val = min(min(y_true), min(y_pred))
        max_val = max(max(y_true), max(y_pred))
        plt.plot([min_val, max_val], [min_val, max_val], 'r--')
        
        plt.xlabel('True Values')
        plt.ylabel('Predictions')
        plt.title(f'True vs Predicted - {model_name}')
        
        plot_path = self.output_dir / f"{model_name}_regression_plot.png"
        plt.savefig(plot_path)
        plt.close()
        
        logger.info(f"Saved regression plot to {plot_path}")

if __name__ == "__main__":
    # Example usage
    evaluator = ModelEvaluator()
    
    # Classification example
    y_true = ['High', 'Medium', 'Low', 'High', 'Critical']
    y_pred = ['High', 'Low', 'Low', 'High', 'High']
    evaluator.evaluate_classifier(y_true, y_pred, model_name='example_classifier')
    
    # Regression example
    y_true_reg = np.array([3.2, 5.7, 2.3, 8.1, 6.4])
    y_pred_reg = np.array([2.9, 5.2, 2.5, 7.8, 5.9])
    evaluator.evaluate_regressor(y_true_reg, y_pred_reg, model_name='example_regressor')
