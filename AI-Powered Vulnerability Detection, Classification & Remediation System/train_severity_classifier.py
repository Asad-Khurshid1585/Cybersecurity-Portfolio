#!/usr/bin/env python3
"""
Train and evaluate different severity classification models.
"""

import argparse
import logging
import sys
import os
import json
from pathlib import Path
from models.severity_classifier import VulnerabilitySeverityClassifier
from utils.evaluation import ModelEvaluator
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Train vulnerability severity classifier')
    parser.add_argument('--data-file', required=True, help='Path to vulnerability data file')
    parser.add_argument('--model-dir', default='models/severity', help='Directory to save models')
    parser.add_argument('--test-size', type=float, default=0.2, help='Test set proportion')
    parser.add_argument('--use-bert', action='store_true', help='Use BERT classifier')
    parser.add_argument('--results-dir', default='results', help='Directory to save results')
    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')
    
    return parser.parse_args()

def main():
    """Train and evaluate severity classification models."""
    args = parse_args()
    
    # Set logging level based on verbosity
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Check and optimize GPU settings
    from utils.gpu_check import check_gpu, optimize_for_gpu_training
    gpu_info = check_gpu()
    if gpu_info["available"]:
        optimize_for_gpu_training()
        logger.info("GPU optimizations applied for training")
    
    # Create output directories
    os.makedirs(args.model_dir, exist_ok=True)
    os.makedirs(args.results_dir, exist_ok=True)
    
    # Initialize the evaluator
    evaluator = ModelEvaluator(args.results_dir)
    
    # Load vulnerability data
    try:
        with open(args.data_file, 'r') as f:
            vulnerability_data = json.load(f)
        logger.info(f"Loaded {len(vulnerability_data)} vulnerabilities from {args.data_file}")
    except Exception as e:
        logger.error(f"Error loading data file: {e}")
        return 1
    
    # Initialize the classifier
    classifier = VulnerabilitySeverityClassifier(model_dir=args.model_dir)
    
    # Train the classifier
    logger.info("Training vulnerability severity classifier...")
    results = classifier.train(
        vulnerability_data, 
        test_size=args.test_size,
        use_bert=args.use_bert
    )
    
    # Save and display results
    if not results:
        logger.error("Training failed to produce results.")
        return 1
    
    # Compare model results
    model_comparison = []
    for model_name, metrics in results.items():
        model_comparison.append({
            'Model': model_name,
            'Accuracy': metrics['accuracy'],
            'Precision': metrics['precision'],
            'Recall': metrics['recall'],
            'F1 Score': metrics['f1']
        })
        
        logger.info(f"\nResults for {model_name}:")
        logger.info(f"Accuracy: {metrics['accuracy']:.4f}")
        logger.info(f"Precision: {metrics['precision']:.4f}")
        logger.info(f"Recall: {metrics['recall']:.4f}")
        logger.info(f"F1 Score: {metrics['f1']:.4f}")
        if 'best_params' in metrics:
            logger.info(f"Best parameters: {metrics['best_params']}")
        
        # Save classification report
        report_path = os.path.join(args.results_dir, f"{model_name}_report.txt")
        with open(report_path, 'w') as f:
            f.write(f"Model: {model_name}\n\n")
            f.write(f"Accuracy: {metrics['accuracy']:.4f}\n")
            f.write(f"Precision: {metrics['precision']:.4f}\n")
            f.write(f"Recall: {metrics['recall']:.4f}\n")
            f.write(f"F1 Score: {metrics['f1']:.4f}\n\n")
            f.write("Classification Report:\n")
            f.write(metrics['classification_report'])
            
            if 'best_params' in metrics:
                f.write("\nBest Parameters:\n")
                f.write(str(metrics['best_params']))
    
    # Create comparison visualizations
    df = pd.DataFrame(model_comparison)
    
    # Plot comparison
    plt.figure(figsize=(12, 8))
    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
    
    # Melt the dataframe for easier plotting
    df_melted = pd.melt(df, id_vars=['Model'], value_vars=metrics_to_plot, 
                         var_name='Metric', value_name='Value')
    
    # Create the grouped bar chart
    sns.barplot(x='Model', y='Value', hue='Metric', data=df_melted)
    plt.title('Performance Comparison of Severity Classification Models')
    plt.ylabel('Score')
    plt.ylim(0, 1.0)
    plt.xticks(rotation=45)
    plt.tight_layout()
    
    # Save the plot
    plot_path = os.path.join(args.results_dir, "model_comparison.png")
    plt.savefig(plot_path)
    plt.close()
    
    logger.info(f"Saved model comparison plot to {plot_path}")
    
    # Save comparison table
    table_path = os.path.join(args.results_dir, "model_comparison.csv")
    df.to_csv(table_path, index=False)
    logger.info(f"Saved model comparison table to {table_path}")
    
    logger.info("Training and evaluation completed successfully.")
    return 0

if __name__ == "__main__":
    sys.exit(main())
