"""
Module for generating remediation suggestions for vulnerabilities.
"""

import requests
import json
import logging
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)

class RemediationGenerator:
    """
    Class to generate remediation suggestions for vulnerabilities,
    using a local LLM API.
    """
    
    def __init__(self, api_url: str = "http://localhost:1234/v1/chat/completions", 
                 model: str = "qwen3-1.7b"):
        """
        Initialize the remediation generator.
        
        Args:
            api_url: URL of the local LLM API
            model: Model name to use
        """
        self.api_url = api_url
        self.model = model
        self.headers = {
            "Content-Type": "application/json"
        }
    
    def get_remediation_suggestion(self, vulnerability: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate remediation suggestions for a vulnerability.
        
        Args:
            vulnerability: Dictionary containing vulnerability information
            
        Returns:
            Dictionary with remediation information
        """
        try:
            # Extract relevant information from vulnerability
            vuln_id = vulnerability.get("cve_id", vulnerability.get("id", "Unknown"))
            title = vulnerability.get("title", "")
            description = vulnerability.get("description", "")
            impact = vulnerability.get("impact", "")
            severity = vulnerability.get("severity", "Unknown")
            
            # Create the prompt for the LLM
            prompt = self._create_prompt(vuln_id, title, description, impact, severity)
            
            # Call the LLM
            response = self._call_llm(prompt)
            
            # Parse the response
            remediation = self._parse_llm_response(response, vuln_id)
            
            return remediation
        except Exception as e:
            logger.error(f"Error generating remediation for {vulnerability.get('cve_id', 'unknown')}: {e}")
            return self._get_generic_remediation()
    
    def _create_prompt(self, vuln_id: str, title: str, description: str, 
                      impact: str, severity: str) -> str:
        """
        Create a prompt for the LLM.
        
        Args:
            vuln_id: Vulnerability ID
            title: Vulnerability title
            description: Vulnerability description
            impact: Vulnerability impact
            severity: Vulnerability severity
            
        Returns:
            Prompt for the LLM
        """
        return f"""You are a cybersecurity expert. Your task is to analyze a vulnerability and provide remediation steps.
        
Vulnerability ID: {vuln_id}
Title: {title}
Description: {description}
Impact: {impact}
Severity: {severity}

Please provide a detailed remediation plan that includes:
1. A short, actionable title for the remediation plan
2. A concise description of the required remediation approach
3. A clear numbered list of specific steps to take
4. Any additional recommendations or warnings

Format your response as:
{{
"title": "Remediation Title",
"description": "Remediation description",
"remediation_steps": ["Step 1", "Step 2", "Step 3"],
"additional_info": "Any extra information"
}}
"""
    
    def _call_llm(self, prompt: str) -> Dict[str, Any]:
        """
        Call the local LLM API.
        
        Args:
            prompt: Prompt to send to the LLM
            
        Returns:
            LLM response
        """
        try:
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": "You are a cybersecurity expert providing remediation advice for vulnerabilities. Respond with concise, actionable remediation steps in JSON format."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3,
                "max_tokens": -1,
                "stream": False
            }
            
            response = requests.post(self.api_url, headers=self.headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()
            
            return response.json()
        except requests.RequestException as e:
            logger.error(f"Error calling LLM API: {e}")
            raise
    
    def _parse_llm_response(self, response: Dict[str, Any], vuln_id: str) -> Dict[str, Any]:
        """
        Parse the LLM response into a remediation.
        
        Args:
            response: LLM response
            vuln_id: Vulnerability ID
            
        Returns:
            Parsed remediation
        """
        try:
            content = response.get('choices', [{}])[0].get('message', {}).get('content', '')
            
            # Try to extract JSON from the response
            try:
                # Find JSON content within the response
                json_start = content.find('{')
                json_end = content.rfind('}') + 1
                
                if json_start >= 0 and json_end > json_start:
                    json_content = content[json_start:json_end]
                    remediation_data = json.loads(json_content)
                else:
                    raise ValueError("No JSON content found in response")
                
            except (json.JSONDecodeError, ValueError):
                # If JSON parsing fails, try to extract structured information from text
                lines = content.split('\n')
                remediation_data = {
                    "title": next((line.split(':', 1)[1].strip() for line in lines if line.lower().startswith('title:')), "Address Vulnerability"),
                    "description": next((line.split(':', 1)[1].strip() for line in lines if line.lower().startswith('description:')), content[:200]),
                    "remediation_steps": [line.strip() for line in lines if line.strip().startswith('- ') or line.strip().startswith('* ')],
                }
            
            # Ensure we have all required fields
            if "remediation_steps" not in remediation_data or not remediation_data["remediation_steps"]:
                remediation_data["remediation_steps"] = ["Research this vulnerability", "Apply appropriate updates"]
                
            if "title" not in remediation_data or not remediation_data["title"]:
                remediation_data["title"] = "Address Vulnerability"
                
            if "description" not in remediation_data or not remediation_data["description"]:
                remediation_data["description"] = "This vulnerability requires attention."
            
            # Add ID to the remediation
            remediation_data["id"] = f"REM-{vuln_id}"
            
            return remediation_data
            
        except Exception as e:
            logger.error(f"Error parsing LLM response: {e}")
            return self._get_generic_remediation(vuln_id)
    
    def _get_generic_remediation(self, vuln_id: str = "GENERIC") -> Dict[str, Any]:
        """
        Get a generic remediation when specific remediation can't be generated.
        
        Args:
            vuln_id: Vulnerability ID
            
        Returns:
            Generic remediation
        """
        return {
            "id": f"REM-{vuln_id}",
            "title": "Review and Address Vulnerability",
            "description": "This vulnerability requires assessment and remediation based on your specific environment.",
            "remediation_steps": [
                "Research this vulnerability to understand its impact",
                "Check for vendor-supplied patches or updates",
                "Apply appropriate security controls",
                "Consider implementing temporary mitigation if no patch is available",
                "Verify that the vulnerability has been resolved after remediation"
            ],
            "additional_info": "Consider consulting security advisories for the most current remediation advice."
        }
