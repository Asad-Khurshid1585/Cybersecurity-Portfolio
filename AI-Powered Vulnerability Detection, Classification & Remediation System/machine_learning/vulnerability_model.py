"""
Module for training and using machine learning models for vulnerability analysis.
"""

import os
import json
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix
import joblib
import logging

logger = logging.getLogger(__name__)

class VulnerabilityModel:
    """
    Train and use machine learning models to analyze vulnerabilities.
    """
    
    def __init__(self, model_dir='models'):
        """
        Initialize the vulnerability model.
        
        Args:
            model_dir: Directory to save/load trained models
        """
        self.model_dir = model_dir
        os.makedirs(model_dir, exist_ok=True)
        self.model = None
    
    def preprocess_data(self, nvd_data):
        """
        Preprocess NVD data for machine learning.
        
        Args:
            nvd_data: List of CVE items from NVD
            
        Returns:
            DataFrame ready for model training
        """
        records = []
        
        for vuln in nvd_data:
            # Skip if empty or missing essential data
            if not vuln:
                continue
                
            record = {}
            
            # Extract CVE ID
            record['cve_id'] = vuln.get('id', '')
            if not record['cve_id']:
                # Try alternative formats
                record['cve_id'] = vuln.get('cve', {}).get('id', '')
            
            if not record['cve_id']:
                # Skip entries without CVE ID
                continue
            
            # Extract metrics data (CVSS)
            # Handle both direct metrics and nested cve.metrics formats
            metrics = None
            if 'metrics' in vuln:
                metrics = vuln['metrics']
            elif 'cve' in vuln and 'metrics' in vuln['cve']:
                metrics = vuln['cve']['metrics']
                
            if not metrics:
                # No metrics data, skip this entry
                continue
            
            # Try to get CVSS v3 data first, fall back to v2
            cvss_data = None
            cvss_version = 3.0
            
            # Handle different CVSS v3 metric keys that might exist
            for metric_key in ['cvssMetricV31', 'cvssMetricV30', 'cvssMetricV3']:
                if metric_key in metrics and metrics[metric_key]:
                    cvss_data = metrics[metric_key][0].get('cvssData', {})
                    break
            
            if not cvss_data:
                # Try to find CVSS v2 metrics
                if 'cvssMetricV2' in metrics and metrics['cvssMetricV2']:
                    cvss_data = metrics['cvssMetricV2'][0].get('cvssData', {})
                    cvss_version = 2.0
                else:
                    # No CVSS data, try some fallbacks before skipping
                    # Sometimes the data might be directly in the metrics object
                    if 'baseScore' in metrics:
                        cvss_data = metrics
                    else:
                        # Skip vulnerabilities without CVSS data
                        continue
            
            # Get base score and categorize severity
            record['base_score'] = cvss_data.get('baseScore', 0)
            record['cvss_version'] = cvss_version
            
            # Categorize severity based on base score
            if record['base_score'] >= 9.0:
                record['severity'] = 'critical'
            elif record['base_score'] >= 7.0:
                record['severity'] = 'high'
            elif record['base_score'] >= 4.0:
                record['severity'] = 'medium'
            else:
                record['severity'] = 'low'
            
            # Extract CVSS metrics with proper fallbacks and defaults
            record['attack_vector'] = cvss_data.get('attackVector', 'UNKNOWN')
            record['attack_complexity'] = cvss_data.get('attackComplexity', cvss_data.get('accessComplexity', 'UNKNOWN'))
            record['confidentiality_impact'] = cvss_data.get('confidentialityImpact', 'UNKNOWN')
            record['integrity_impact'] = cvss_data.get('integrityImpact', 'UNKNOWN')
            record['availability_impact'] = cvss_data.get('availabilityImpact', 'UNKNOWN')
            
            # Add publication and last modified dates
            record['published'] = vuln.get('published', '')
            if not record['published'] and 'cve' in vuln:
                record['published'] = vuln['cve'].get('published', '')
                
            record['lastModified'] = vuln.get('lastModified', '')
            if not record['lastModified'] and 'cve' in vuln:
                record['lastModified'] = vuln['cve'].get('lastModified', '')
            
            records.append(record)
        
        if not records:
            logger.warning("No valid records for training")
            return None
            
        df = pd.DataFrame(records)
        
        # Convert dates to datetime and extract features
        for date_col in ['published', 'lastModified']:
            if date_col in df.columns:
                try:
                    df[date_col] = pd.to_datetime(df[date_col])
                    df[f'{date_col}_year'] = df[date_col].dt.year
                    df[f'{date_col}_month'] = df[date_col].dt.month
                    df[f'{date_col}_day'] = df[date_col].dt.day
                except (ValueError, TypeError):
                    # Handle date conversion errors
                    logger.warning(f"Could not convert {date_col} to datetime. Using default values.")
                    df[f'{date_col}_year'] = 2022  # Default year
                    df[f'{date_col}_month'] = 1    # Default month
                    df[f'{date_col}_day'] = 1      # Default day
        
        return df
    
    def train(self, nvd_data, target='severity'):
        """
        Train a model to predict vulnerability severity.
        
        Args:
            nvd_data: List of CVE items from NVD
            target: Target variable to predict (default: 'severity')
            
        Returns:
            Trained model
        """
        logger.info(f"Training model on {len(nvd_data)} vulnerabilities")
        
        df = self.preprocess_data(nvd_data)
        if df is None or df.empty:
            logger.error("No data available for training")
            return None
        
        logger.info(f"Preprocessed data shape: {df.shape}")
        
        # Define features
        categorical_features = [
            'attack_vector', 
            'attack_complexity',
            'confidentiality_impact',
            'integrity_impact',
            'availability_impact'
        ]
        
        numeric_features = [
            'base_score',
            'cvss_version',
        ]
        
        # Add date features if available
        date_features = [
            'published_year',
            'published_month',
            'published_day',
            'lastModified_year',
            'lastModified_month',
            'lastModified_day'
        ]
        
        # Only use features that exist in the data
        numeric_features = [f for f in numeric_features if f in df.columns]
        categorical_features = [f for f in categorical_features if f in df.columns]
        date_features = [f for f in date_features if f in df.columns]
        
        # Combine all numeric features
        all_numeric_features = numeric_features + date_features
        
        # Check if we have the target column
        if target not in df.columns:
            logger.error(f"Target column '{target}' not found in data")
            return None
        
        # Check if we have enough distinct target values for classification
        if len(df[target].unique()) < 2:
            logger.error(f"Not enough distinct values in target '{target}' for classification")
            return None
        
        # Prepare preprocessing steps
        transformers = []
        
        if all_numeric_features:
            transformers.append(('num', StandardScaler(), all_numeric_features))
            
        if categorical_features:
            transformers.append(('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features))
        
        if not transformers:
            logger.error("No valid features found for model training")
            return None
            
        preprocessor = ColumnTransformer(transformers=transformers)
        
        # Create the model pipeline
        pipeline = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
        ])
        
        # Train-test split
        try:
            X = df.drop([target, 'cve_id'], axis=1, errors='ignore')
            y = df[target]
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # Train the model
            pipeline.fit(X_train, y_train)
            
            # Evaluate the model
            y_pred = pipeline.predict(X_test)
            logger.info("Model evaluation:")
            logger.info("\n" + classification_report(y_test, y_pred))
            
            # Save the model
            model_path = os.path.join(self.model_dir, 'vulnerability_classifier.joblib')
            joblib.dump(pipeline, model_path)
            logger.info(f"Model saved to {model_path}")
            
            self.model = pipeline
            return pipeline
            
        except Exception as e:
            logger.error(f"Error during model training: {e}")
            return None
    
    def predict(self, vulnerabilities):
        """
        Predict severity for new vulnerability data.
        
        Args:
            vulnerabilities: List of vulnerability dictionaries
            
        Returns:
            List of predictions
        """
        if self.model is None:
            model_path = os.path.join(self.model_dir, 'vulnerability_classifier.joblib')
            if os.path.exists(model_path):
                self.model = joblib.load(model_path)
            else:
                raise ValueError("No trained model available. Train a model first.")
        
        df = self.preprocess_data(vulnerabilities)
        if df is None or df.empty:
            return []
        
        X = df.drop(['severity', 'cve_id'], axis=1, errors='ignore')
        predictions = self.model.predict(X)
        
        # Create result with predictions
        results = []
        for i, pred in enumerate(predictions):
            results.append({
                'cve_id': df.iloc[i].get('cve_id', f'Unknown-{i}'),
                'predicted_severity': pred,
                'base_score': df.iloc[i].get('base_score', 0)
            })
        
        return results


# Example usage if run directly
if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s"
    )
    
    # Example data
    sample_data = [
        {
            "id": "CVE-2021-44228",
            "descriptions": [{"lang": "en", "value": "Log4j vulnerability"}],
            "metrics": {
                "cvssMetricV31": [
                    {
                        "cvssData": {
                            "baseScore": 10.0,
                            "attackVector": "NETWORK",
                            "attackComplexity": "LOW",
                            "confidentialityImpact": "HIGH",
                            "integrityImpact": "HIGH",
                            "availabilityImpact": "HIGH"
                        }
                    }
                ]
            },
            "published": "2021-12-10T10:15:00.000",
            "lastModified": "2021-12-16T21:12:00.000"
        }
    ]
    
    # Create and train model
    model = VulnerabilityModel()
    model.train(sample_data)
    
    # Make predictions
    predictions = model.predict(sample_data)
    print("Predictions:", predictions)
